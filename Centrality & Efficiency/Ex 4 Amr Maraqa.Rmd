---
title: "ONA_Ex_4_Amr_Maraqa"
author: "Amr Maraqa"
date: "2023-04-02"
output:
  md_document: default
  pdf_document: default
knit: (function(inputFile, encoding) {
  rmarkdown::render(inputFile, encoding = encoding, output_format = "all")})
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
library(knitr)
library(here)
library(arrow)
library(gender)
library(lubridate)

library(tidyverse)
library(igraph)
library(tidygraph)
library(ggraph)
library(gridExtra)
```

# Section 1: Data Preprocessing

```{r load-data, echo=FALSE}
# change to your own path!
data_path <- "C:\\Users\\m.maraqa\\Desktop\\MMA courses\\Winter 2023\\Organizational Network Analysis\\Exercise 3\\"
applications <- read_parquet(paste0(data_path,"app_data_sample.parquet"))
edges <- read_csv(paste0(data_path,"edges_sample.csv"))

kable(applications[1:10,], caption = 'Applications Data')
```
## 1.1. Examiner's Gender

We'll get gender based on the first name of the examiner, which is recorded in the field `examiner_name_first`. We'll use library `gender` for that, relying on a modified version of their own [example](https://cran.r-project.org/web/packages/gender/vignettes/predicting-gender.html).

Note that there are over 2 million records in the applications table -- that's because there are many records for each examiner, as many as the number of applications that examiner worked on during this time frame. Our first step therefore is to get all *unique* names in a separate list `examiner_names`. We will then guess gender for each one and will join this table back to the original dataset. So, let's get names without repetition:

```{r gender-1, echo=FALSE}
library(gender)
#install_genderdata_package() # only run this line the first time you use the package, to get data for it

# get a list of first names without repetitions
examiner_names <- applications %>% 
  distinct(examiner_name_first)

kable(examiner_names, caption = 'Examiner Names')
```
Now let's use function `gender()` as shown in the example for the package to attach a gender and probability to each name and put the results into the table `examiner_names_gender`

```{r gender-2, echo=FALSE}
# get a table of names and gender
examiner_names_gender <- examiner_names %>% 
  do(results = gender(.$examiner_name_first, method = "ssa")) %>% 
  unnest(cols = c(results), keep_empty = TRUE) %>% 
  select(
    examiner_name_first = name,
    gender,
    proportion_female
  )
examiner_names_gender
kable(head(examiner_names_gender, 10), caption = 'Predicted Examiner Gender')
```
Finally, let's join that table back to our original applications data and discard the temporary tables we have just created to reduce clutter in our environment.

```{r gender-3, echo=FALSE}
# remove extra colums from the gender table
examiner_names_gender <- examiner_names_gender %>% 
  select(examiner_name_first, gender)

# joining gender back to the dataset
applications <- applications %>% 
  left_join(examiner_names_gender, by = "examiner_name_first")

# cleaning up
rm(examiner_names)
rm(examiner_names_gender)
gc()

```
## 1.2. Examiner's Race

We'll now use package `wru` to estimate likely race of an examiner. Just like with gender, we'll get a list of unique names first, only now we are using surnames.

```{r race-1, echo=FALSE}
library(wru)

examiner_surnames <- applications %>% 
  select(surname = examiner_name_last) %>% 
  distinct()

kable(head(examiner_surnames, 10), caption = 'Examiner Surnames')
```
We'll follow the instructions for the package outlined here [https://github.com/kosukeimai/wru](https://github.com/kosukeimai/wru).

```{r race-2, echo=FALSE}
examiner_race <- predict_race(voter.file = examiner_surnames, surname.only = T) %>% 
  as_tibble()
```

As you can see, we get probabilities across five broad US Census categories: white, black, Hispanic, Asian and other. (Some of you may correctly point out that Hispanic is not a race category in the US Census, but these are the limitations of this package.)

Our final step here is to pick the race category that has the highest probability for each last name and then join the table back to the main applications table. See this example for comparing values across columns: [https://www.tidyverse.org/blog/2020/04/dplyr-1-0-0-rowwise/](https://www.tidyverse.org/blog/2020/04/dplyr-1-0-0-rowwise/). And this one for `case_when()` function: [https://dplyr.tidyverse.org/reference/case_when.html](https://dplyr.tidyverse.org/reference/case_when.html).

```{r race-3, echo=FALSE}
examiner_race <- examiner_race %>% 
  mutate(max_race_p = pmax(pred.asi, pred.bla, pred.his, pred.oth, pred.whi)) %>% 
  mutate(race = case_when(
    max_race_p == pred.asi ~ "Asian",
    max_race_p == pred.bla ~ "black",
    max_race_p == pred.his ~ "Hispanic",
    max_race_p == pred.oth ~ "other",
    max_race_p == pred.whi ~ "white",
    TRUE ~ NA_character_
  ))

kable(head(examiner_race, 10), caption = 'Predicted Examiner Race')

```
Let's join the data back to the applications table.

```{r race-4, echo=FALSE}
# removing extra columns
examiner_race <- examiner_race %>% 
  select(surname,race)

applications <- applications %>% 
  left_join(examiner_race, by = c("examiner_name_last" = "surname"))

rm(examiner_race)
rm(examiner_surnames)
gc()
```

## 1.3. Examiner's Tenure 

To figure out the timespan for which we observe each examiner in the applications data, let's find the first and the last observed date for each examiner. We'll first get examiner IDs and application dates in a separate table, for ease of manipulation. We'll keep examiner ID (the field `examiner_id`), and earliest and latest dates for each application (`filing_date` and `appl_status_date` respectively). We'll use functions in package `lubridate` to work with date and time values.

```{r tenure-1, echo=FALSE}
library(lubridate) # to work with dates

examiner_dates <- applications %>% 
  select(examiner_id, filing_date, appl_status_date) 

kable(head(examiner_dates, 10), caption = 'Examiner Dates')

```

The dates look inconsistent in terms of formatting. Let's make them consistent. We'll create new variables `start_date` and `end_date`.

```{r tenure-2, echo=FALSE}
examiner_dates <- examiner_dates %>% 
  mutate(start_date = ymd(filing_date), end_date = as_date(dmy_hms(appl_status_date)))
```

Let's now identify the earliest and the latest date for each examiner and calculate the difference in days, which is their tenure in the organization.

```{r tenure-3, echo=FALSE}
examiner_dates <- examiner_dates %>% 
  group_by(examiner_id) %>% 
  summarise(
    earliest_date = min(start_date, na.rm = TRUE), 
    latest_date = max(end_date, na.rm = TRUE),
    tenure_days = interval(earliest_date, latest_date) %/% days(1)
    ) %>% 
  filter(year(latest_date)<2018)%>%
  mutate(tenure_years = tenure_days / 365) %>%
    mutate(tenure = case_when(
      tenure_years <= 1 ~ '<1',
      tenure_years <= 2 ~ '1-2',
      tenure_years <= 5 ~ '3-5',
      tenure_years <= 9 ~ '6-9',
      tenure_years <= 14 ~ '10-14',
      tenure_years <= 100 ~ '15+',
      TRUE ~ NA_character_
    ))

kable(head(examiner_dates, 10), caption = 'Examiner Tenure')
```
Joining back to the applications data.

```{r tenure-4, echo=FALSE}
applications <- applications %>% 
  left_join(examiner_dates, by = "examiner_id")

#dropping NA values in app data
applications <- applications %>%
  drop_na(gender, race, tenure_days)

rm(examiner_dates)
gc()
```
# Section 2: Application Processing Time
The time taken to either issue a patent or abandon an application is the target variable, as it offers a way to measure the efficiency of examiners. The processing time was obtained by calculating the number of days between the date of filing the application to that when the patent is granted or denied. 
```{r app_proc_time calculation, echo=FALSE}
proc_app <- applications %>% 
  filter(applications$disposal_type != 'PEND')

proc_app$proc_time <- ifelse(is.na(proc_app$abandon_date), difftime(proc_app$patent_issue_date, proc_app$filing_date, unit="days"), difftime(proc_app$abandon_date, proc_app$filing_date, unit="days"))

# Drop negative processing time
proc_app <- proc_app %>%
  filter(proc_app$proc_time > 0)

kable(proc_app[1:10, c('disposal_type', 'filing_date', 'patent_issue_date', 'abandon_date', 'proc_time')], caption = 'Application Processing Time')
```

# Section 3: Network Metrics
Since the impact of an examiner's centrality on his/her efficiency is the relationship in question, the centrality metrics must be calculated. The measures of degree, betweenness and closeness were obtained for the purpose of this exercise.
```{r Create Network, echo=FALSE}
edg <- edges %>% 
  drop_na() %>% 
  select(to = ego_examiner_id, from = alter_examiner_id)

network <- graph_from_data_frame(edg, directed = TRUE) %>%
  as_tbl_graph() 
```

```{r Network Metrics, echo=FALSE}
network <- network %>%
  mutate(degree = centrality_degree(),
         between = centrality_betweenness(),
         close = centrality_closeness())

network_data <- network %>% as.data.frame() %>% as_tibble() %>% rename(examiner_id = name)

proc_app$examiner_id <- as.character(proc_app$examiner_id)
proc_app <- proc_app %>% left_join(network_data, by = 'examiner_id')

kable(proc_app[1:10, c('examiner_id', 'degree', 'between', 'close', 'proc_time')], caption = 'Network Metrics')
```
# Section 4: Linear Regression
A linear regression is performed to study the significance of examiner centrality in the USPTO network on application processing time. The subsequent sections supplement the simple linear regression with other factors to examine the difference in effects ensued by them.

## 4.1. Processing Time and Centrality
```{r linear regression, echo=False}
# Drop NAs in degree
sum(is.na(proc_app$degree)) # 623692 NAs

proc_app <- proc_app %>% 
  filter(!is.na(proc_app$degree))

attach(proc_app)
fit1 <- lm(proc_time~degree + between + close)

summary(fit1)

kable(summary(fit1)$coefficients, caption='Linear Regression: Processing Time and Centrality')
```
  As shown in the table above, the relationship between the measures of centrality and examiner efficiency is highly significant. The results show that a unit increase in examiner's measure of betweenness increases processing time by several minutes. The other measures, on the other hand, have a positive and a much more meaningful impact on efficiency, with closeness affecting improving efficiency more intensely than degree centrality. 

  The nature of the relationships between the measures of centrality and efficiency align with commonly held expectations for social dynamics. Degree centrality represents popularity of an examiner within the USPTO. Its relationship with application processing time follows the logic that if an examiner is more popular, he/she would know more people to help with the examination process, thus shortening the time required to complete the process, on average. It could also mean that people want to associate with the popular examiners in the organization, and are, therefore, more willing to help. The effect of closeness follows the same explanation, but to a much greater extent. This may be attributed to the tightness of the an examiner's cluster to him/her when his/her closeness centrality is high, which means that others are more willing to contribute due to the strength of their relationships with the examiner.
  
  Although a high betweenness gives an examiner more influence on the flow of information within networks and higher exposure to different examiner circles, an increase in betweenness slightly harms efficiency. This may be due to the delay that occurs from the necessity for an application to be transferred from one side of an examiner's network to the other, as the surrounding examiners are not directly connected and must funnel their communication through their link.

## 4.2. Degree Significance Difference: Gender 
```{r linear regression gender interaction, echo=False}
# Drop NAs in degree
sum(is.na(proc_app$degree)) # 623692 NAs

proc_app <- proc_app %>% 
  filter(!is.na(proc_app$degree))

attach(proc_app)
fit2 <- lm(proc_time ~ degree + between + close + gender + gender*(degree + between + close))
summary(fit2)
kable(summary(fit2)$coefficients, caption='Linear Regression: Gender Impact')
```
The results of the second linear regression model show that male examiners, on average, take around 47 more days to process an application than female examiners do. However, the increases in efficiency caused by centrality measures are higher for male examiners. Although the reason remains unclear, this may be an indication of the existence of sexism within USPTO, where popularity, comradeship and connections are more meaningful to men at work than women. 

## 4.3. Degree Significance Difference: Race
```{r linear regression race interaction, echo=False}
# Drop NAs in degree
sum(is.na(proc_app$degree)) # 623692 NAs

proc_app <- proc_app %>% 
  filter(!is.na(proc_app$degree))

attach(proc_app)
fit3 <- lm(proc_time ~ degree + between + close + race + race*(degree + between + close))
summary(fit3)
kable(summary(fit3)$coefficients, caption='Linear Regression: Race Impact')
```
  The numbers in the table above show that, compared to Asian examiners, examiners of other races are generally more efficient. However, the effects of centrality on Asian examiners are better than those for examiners of other races. Moreover, the nature of the relationships between centrality and efficiency for the different races defy the logic used to explain the observed behavior of efficiency in response to centrality previously. This can be a sign of racial discrimination among examiners, as increases in the size and quality of an examiner's network seem to harm his/her efficiency based on his/her race.

# Conclusion
  In this exercise, the impact of an examiner's centrality on the processing time of his/her assigned applications was studied. The results of the linear regression models show that these measures have significant relationships with the efficiency of examiners and that the nature of these relationships can be soundly justified. The results also signify to the USPTO that there may exist discrimination based on gender and race within its organization. 
  
  The insights that USPTO's management can draw from these results is that degree and closeness are favorable for efficiency. Therefore, holding events that solidify relationships among examiners may prove worthwhile. These events can also help to alleviate the problems pertaining to racial and gender discrimination. The numbers also draw the management's attention to the importance of eliminating data and communication silos, as the existence of only one funnel of communication between examiners renders the application processing more time consuming. The solution to these isolated entities can be in the establishment of decentralized data systems, like cloud infrastructures, and the migration of communication systems to these infrastructures as well. 
  
  It's worth to note, however, that these regression models have a very low R-squared, which means they can't explain the variance in application processing time. Thus, if managers want to predict efficiency, they are advised to use more advanced regression techniques.
